Q1 because such implementations could slow down performance especially since you have to create these data structures which takes time and since they’re not used they don’t accuratly reflect the performance of that particular implementationQ2 pretty much the same difficulty just identify where to synchronizeQ3 Technically speaking it is possible to implement this list-level locking without touching the hash but only for this hash since we know that the total number of buckets is constant. Since we have this number we can technically create my_size number of locks and do it like that without touching the hash. However this is a really messy method and relies on hard coding the number of locks so it would not be advisable. It is much better to implement the locks inside the hash.h file. Either case though you will need to have knowledge of the internal implementation as knowledge of the number of buckets is need to create a list level lock, since you need a lock for each bucket in the hash.Q4 This implementation does not work since you do not lock when you try to increment the count which is also part of the critical section. So when two threads try and increment the count at the same time since it’s not locked it can cause the value to be incorrect.Q5 This like the above implementation also does not lock the count and so will not workQ6 This method works because you can create these two methods which by taking in the specific key and mapping it the the hash-index can lock on the bucket that the element is going to effect. This also accounts for the count increment issue as you can specifically call these two methods around the critical section as we did for the global lock but this time it will lock it on each list instead of globally. This will therefore lock around the count increment as well so it should work.Q7 TM is much easier to implement as all you have to do is surround the critical section with the tm block wheras with the list-level lock you need to actually add in specific implementation that creates locks for each bucket.Q8 the pros of this approach are that it is significantly faster then using locks since by doing it this way none of the threads need to wait for the other thread to finish working with the resource and pass on the lock and merging is significantly shorter than the total amount of wait time spent on acquiring the lock. However this approach also consumes a lot more memory in my implementation it consume memory proportional to the number of threads that are created as well as an additional one for the output hash. Therefore for four thread it requires 4(local hashes) + 1(output hash) times more then with the non reduction method.Implementation1 Thread2 Threads4 ThreadsGlobal Lock19.1413.6721.07TM21.0820.6313.42List Lock19.6210.5806.86Element Lock19.8510.2507.18Reduction17.5608.8704.45Original randtrack: 17.60Q9ImplementationOverheadGlobal Lock1.0875TM1.197727273List Lock1.114772727Element Lock1.127840909Reduction0.997727273Q10 for the most part the performance increases as the number of threads increases which makes sense since its more highly panellized. However we can see for the global lock that that for 4 threads it is actually slower than even the single threaded implementation. This can be explained because with global locks they it will lock the entire critical section block can only be accessed by one thread at any given time so all the other threads end up just waiting while only one thread can work on inserting and incrementing the counter. This pretty much defeats the purpose of creating additional threads as no matter how many is created the section that actually does the work (ie. insert and lookup and increment) will still only be worked on by one thread at a time and in the end it just creates a lot of overhead which ends up even slowing down the performance past that of even its single threaded counterpart.Q11Implementation1 Thread2 Threads4 ThreadsGlobal Lock36.3022.1218.93TM38.4029.1216.77List Lock37.0119.2811.19Element Lock37.1718.8511.28Reduction34.7417.4008.74Original randtrack: 34.73We can see that compared with to 50 the impact of increasing the number of threads is a lot larger when we set it to 100 this makes sense since more work is being done since each time it must randomize it 100 times rather than 50. Therefore parallelizing it would have a much larger impact then in the case when 50. This is also cause partially by the fact that most of the extra work from changing samples to skip to 100 that is being done is being done in the non-critical section therefore it does not run into the issue of waiting to acquire a lock this also explains why the impact of increasing the number of threads is higher for the 100 case. Also we can see that unlike for 50 the global lock for the case of 4 threads does not become slower then the case with 1 thread. This is also explained by the fact that since we an increase of work in the section which does not require locking each individual thread will spend more time in that section therefore the portion of time spent on waiting to acquire a lock is reduced and so we get the observed results.Q12 The approach that OptsRus should probably ship depends mainly on the memory capacity in relation to the number of cores of the customer’s system. This is because the fastest solution appears to be reduction however since it requires a significantly larger amount of memory than all the other implementations it may not be feasible for extremely low memory systems that have a large number of cores. However if this is not the case reduction would be best otherwise from the experiments above it would appear that list-level locking would be best at least from the way that it has been implemented, but I suspect a more efficient implementation of element-level locking could be even better. So either of the two could work but only choose these over reduction when the memory on the system is relatively scarce.